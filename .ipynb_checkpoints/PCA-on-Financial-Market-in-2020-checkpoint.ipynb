{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f5dd42",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "A hypothesis behind econometric analysis is that financial market is multivariate. However, this paper intends to challenge this hypothesis, as COVID-19 influenced almost every part of our lives in 2020. I suppose that financial market is mostly influenced by COVID-19, which is that I would like to prove. In macroeconomic perspective, emergence of COVID-19 has led to a global shutdown, collapsing supply and demand of labor market. Financial market also experienced sudden collapse in March, which was soon recovered in extremely fast and linear (even exponential) pace. Based on this truth, I would like to build a null hypothesis ($H_{0}$) that a model to predict price fluctuation of financial market could be uni-variate in the year of 2020 due to massive spread of COVID-19.\n",
    "\n",
    "# Literature Review\n",
    "\n",
    "To show COVID-19 as an only variable to explain price fluctuation, I referred a paper of Kritzman et al. (2010): __'Principal Components as a Measure of Systemic Risk' [[1]](https://doi.org/10.3905/jpm.2011.37.4.112)__. Paper uses Principal Component Analysis to understand systemic risk of financial market. It refers to a ratio of systematic risk to idiosyncratic risk, and principal component analysis extracts principal components, or simplified eigenvectors, as orthogonal transformation of correlated features. That being said, larger __explained variance__ of first two principal components implies that variables regarding market is strongly correlated; market fragility is warned. If there is a single problem that occurs in the part economy, market fragility indicated by larger explained variance implies its impact will be much bigger. As a quote, Kritzman et al. (2010) states that: \n",
    "\n",
    "> \"the absorption ratio, which equals the fraction of the total variance of a set of asset returns explained or “absorbed” by a fixed number of eigenvectors. The absorption ratio captures the extent to which markets are unified or tightly coupled. When markets are tightly coupled, they are more fragile in the sense that negative shocks propagate more quickly and broadly than when markets are loosely linked\". It is my intention that I take principal components from data 'COVID-19 Confirmed' and 'Daily Return' so that we observe how much variance is explained by first principal component. I will extend the principal component into five, so that I can check the trend of the degree of explained variance in detail.\n",
    "\n",
    "# Methodology\n",
    "\n",
    "## Explained Variance\n",
    "\n",
    "To apply Principal Component Analysis, I build a dataframe as a combination of 40 Global Market Indices and Coronavirus Confirmed Rate in specific country. Using a few principal components, I want to calculate `explained variance` to recognize how _closely_ `Confirmed Rate` and `Price Fluctuation` are correlated. If my $H_{0}$ gets rejected, as confirmed rate of COVID-19 and market price fluctuation does not move together, explained variance calculated should be smaller than expected.\n",
    "\n",
    "## Comparing Eigenvector\n",
    "\n",
    "Because Principal Component Analysis reduces dimensionality by taking orthogonal transformation, which is an eigenvector for every confirmed rate. If a value of eigenvector, or __eigenvalue__, is bigger, it implies that $x_{i}$ regarding this eigenvector has more influence over the modeling process. In other words, it ranks how well-correlated each variable is.\n",
    "\n",
    "# Experiment\n",
    "\n",
    "## Preparation for Experiment\n",
    "\n",
    "* Load Packages\n",
    "* Define Functions\n",
    "    * To calculate z-score (normalize into gaussian curve)\n",
    "    * To convert date format (to make date format be identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d23414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 4800x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Packages\n",
    "\n",
    "import yfinance as yf\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from countryinfo import CountryInfo\n",
    "from sklearn.decomposition import PCA\n",
    "from datapackage import Package\n",
    "\n",
    "today = datetime.today()\n",
    "yesterday = str(today - timedelta(2))[:10]\n",
    "\n",
    "# apply the z-score method in Pandas using the .mean() and .std() methods\n",
    "def z_score(df):\n",
    "    # copy the dataframe\n",
    "    df_std = df.copy()\n",
    "    # apply the z-score method\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "        \n",
    "    return df_std\n",
    "\n",
    "# Convert Date\n",
    "def date_convert(dates):\n",
    "    dates_return = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date = date.split(\"/\")\n",
    "        year = '20' + str(date[2])\n",
    "        month = str(date[0])\n",
    "        day = str(date[1])\n",
    "        \n",
    "        if int(month) < 10:\n",
    "            month = '0' + month\n",
    "        \n",
    "        if int(day) < 10:\n",
    "            day = '0' + day\n",
    "        \n",
    "        date = year + \"-\" + month + \"-\" + day\n",
    "        dates_return.append(date)\n",
    "    \n",
    "    return dates_return\n",
    "\n",
    "start = \"2020-01-22\"\n",
    "end = \"2021-01-01\"\n",
    "\n",
    "# Matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('max_rows', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 12)\n",
    "plt.style.use('seaborn-pastel')\n",
    "plt.rcParams['lines.linewidth'] = 1\n",
    "plt.figure(dpi=300)\n",
    "plt.rcParams['lines.color'] = 'b'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba85323",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "### Import Stock Indices\n",
    "\n",
    "This part of code imports __40 Market Indices__ that represent major financial market in the globe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cfbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 40 Market Indices\n",
    "\n",
    "BUK100P = yf.download(\"^BUK100P\", start, end)['Adj Close'].to_frame()\n",
    "SPY = yf.download(\"SPY\", start, end)['Adj Close'].to_frame()\n",
    "Singapore = yf.download(\"^STI\", start, end)['Adj Close'].to_frame()\n",
    "Dow = yf.download(\"^DJI\", start, end)['Adj Close'].to_frame()\n",
    "Nasdaq = yf.download(\"^IXIC\", start, end)['Adj Close'].to_frame()\n",
    "FTSE100 = yf.download(\"^FTSE\", start, end)['Adj Close'].to_frame()\n",
    "FTSE250 = yf.download(\"^FTSE\", start, end)['Adj Close'].to_frame()\n",
    "FTSE350 = yf.download(\"^FTLC\", start, end)['Adj Close'].to_frame()\n",
    "FTAI = yf.download(\"^FTAI\", start, end)['Adj Close'].to_frame()\n",
    "N225 = yf.download(\"^N225\", start, end)['Adj Close'].to_frame()\n",
    "N500 = yf.download(\"^N500\", start, end)['Adj Close'].to_frame()\n",
    "N1000 = yf.download(\"^N1000\", start, end)['Adj Close'].to_frame()\n",
    "HSI = yf.download(\"^HSI\", start, end)['Adj Close'].to_frame()\n",
    "Taiwan = yf.download(\"^TWII\", start, end)['Adj Close'].to_frame()\n",
    "SSE = yf.download(\"000001.SS\", start, end)['Adj Close'].to_frame()\n",
    "Shenzhen = yf.download(\"399001.SZ\", start, end)['Adj Close'].to_frame()\n",
    "DAX = yf.download(\"^GDAXI\", start, end)['Adj Close'].to_frame()\n",
    "France = yf.download(\"^FCHI\", start, end)['Adj Close'].to_frame()\n",
    "Indonesia = yf.download(\"^JKSE\", start, end)['Adj Close'].to_frame()\n",
    "PSEI = yf.download(\"PSEI.PS\", start, end)['Adj Close'].to_frame()\n",
    "AORD = yf.download(\"^AORD\", start, end)['Adj Close'].to_frame()\n",
    "AXJO = yf.download(\"^AXJO\", start, end)['Adj Close'].to_frame()\n",
    "AXKO = yf.download(\"^AXKO\", start, end)['Adj Close'].to_frame()\n",
    "kospi = yf.download(\"^KS11\", start, end)['Adj Close'].to_frame()\n",
    "India = yf.download(\"^BSESN\", start, end)['Adj Close'].to_frame()\n",
    "NZ50 = yf.download(\"^NZ50\", start, end)['Adj Close'].to_frame()\n",
    "XAX = yf.download(\"^XAX\", start, end)['Adj Close'].to_frame()\n",
    "RUI = yf.download(\"^RUI\", start, end)['Adj Close'].to_frame()\n",
    "RUT = yf.download(\"^RUT\", start, end)['Adj Close'].to_frame()\n",
    "RUA = yf.download(\"^RUA\", start, end)['Adj Close'].to_frame()\n",
    "GSPTSE = yf.download(\"^GSPTSE\", start, end)['Adj Close'].to_frame()\n",
    "N100 = yf.download(\"^N100\", start, end)['Adj Close'].to_frame()\n",
    "N150 = yf.download(\"^N150\", start, end)['Adj Close'].to_frame()\n",
    "BFX = yf.download(\"^BFX\", start, end)['Adj Close'].to_frame()\n",
    "IMOEX = yf.download(\"IMOEX.ME\", start, end)['Adj Close'].to_frame()\n",
    "MERV = yf.download(\"^MERV\", start, end)['Adj Close'].to_frame()\n",
    "TA125 = yf.download(\"^TA125.TA\", start, end)['Adj Close'].to_frame()\n",
    "JN0U = yf.download(\"^JN0U.JO\", start, end)['Adj Close'].to_frame()\n",
    "AEX = yf.download(\"^AEX\", start, end)['Adj Close'].to_frame()\n",
    "ATOI = yf.download(\"^ATOI\", start, end)['Adj Close'].to_frame()\n",
    "BVSP = yf.download(\"^BVSP\", start, end)['Adj Close'].to_frame()\n",
    "MIB = yf.download(\"FTSEMIB.MI\", start, end)['Adj Close'].to_frame()\n",
    "ATX = yf.download(\"^ATX\", start, end)['Adj Close'].to_frame()\n",
    "ISEQ = yf.download(\"^ISEQ\", start, end)['Adj Close'].to_frame()\n",
    "NSEI = yf.download(\"^NSEI\", start, end)['Adj Close'].to_frame()\n",
    "MXX = yf.download(\"^MXX\", start, end)['Adj Close'].to_frame()\n",
    "SSMI = yf.download(\"^SSMI\", start, end)['Adj Close'].to_frame()\n",
    "STOXX50E = yf.download(\"^STOXX50E\", start, end)['Adj Close'].to_frame()\n",
    "MDAXI = yf.download(\"^MDAXI\", start, end)['Adj Close'].to_frame()\n",
    "SDAXI = yf.download(\"^SDAXI\", start, end)['Adj Close'].to_frame()\n",
    "HSCC = yf.download(\"^HSCC\", start, end)['Adj Close'].to_frame()\n",
    "HSCE = yf.download(\"^HSCE\", start, end)['Adj Close'].to_frame()\n",
    "KLSE = yf.download(\"^KLSE\", start, end)['Adj Close'].to_frame()\n",
    "\n",
    "# Transform into Dataframe\n",
    "df = pd.concat([\n",
    "    BUK100P,\n",
    "    Dow, \n",
    "    Nasdaq, \n",
    "    FTSE100, \n",
    "    FTSE250, \n",
    "    FTAI, \n",
    "    N225, \n",
    "    SSE, \n",
    "    Shenzhen, \n",
    "    DAX, \n",
    "    France, \n",
    "    Indonesia, \n",
    "    PSEI, \n",
    "    AXKO,\n",
    "    kospi, \n",
    "    NZ50,\n",
    "    RUI, \n",
    "    RUT, \n",
    "    RUA, \n",
    "    GSPTSE,  \n",
    "    N100, \n",
    "    N150, \n",
    "    BFX, \n",
    "    IMOEX, \n",
    "    MERV, \n",
    "    TA125, \n",
    "    JN0U, \n",
    "    SPY, \n",
    "    Singapore, \n",
    "    AEX, \n",
    "    ATOI,\n",
    "    BVSP,\n",
    "    MIB,\n",
    "    ATX,\n",
    "    ISEQ,\n",
    "    MXX,\n",
    "    STOXX50E,\n",
    "    MDAXI,\n",
    "    SDAXI,\n",
    "    KLSE\n",
    "], axis=1)\n",
    "\n",
    "# Set Columns\n",
    "# Columns include Country Code, so that I can match country to COVID-19 confirmed rate.\n",
    "df.columns=[\n",
    "    'US-Dow', \n",
    "    'US-Nasdaq', \n",
    "    'GB-FTSE100', \n",
    "    'GB-FTSE250', \n",
    "    'GB-FTAI',\n",
    "    'GB-BUK100P',\n",
    "    'JP-N225', \n",
    "    'CN-SSE', \n",
    "    'CN-Shenzhen', \n",
    "    'DE-DAX', \n",
    "    'FR-FCHI', \n",
    "    'ID-JKSE', \n",
    "    'PH-PSEI', \n",
    "    'AU-AXKO',\n",
    "    'KR-KSII', \n",
    "    'NZ-NZ50',\n",
    "    'US-RUI', \n",
    "    'US-RUT', \n",
    "    'US-RUA', \n",
    "    'CA-GSPTSE', \n",
    "    'FR-N100', \n",
    "    'FR-N150', \n",
    "    'BE-BFS', \n",
    "    'RU-IMOEX', \n",
    "    'AR-MERV', \n",
    "    'IL-TA125', \n",
    "    'ZA-JN0U', \n",
    "    'US-SPX', \n",
    "    'SG-STI', \n",
    "    'NL-AEX', \n",
    "    'AU-ATOI',\n",
    "    'BR-BVSP',\n",
    "    'IT-MIB',\n",
    "    'AT-ATX',\n",
    "    'IE-ISEQ',\n",
    "    'MX-MXX',\n",
    "    'DE-Stoxx50E',\n",
    "    'DE-MDAXI',\n",
    "    'DE-SDAXI',\n",
    "    'MY-KLSE'\n",
    "]\n",
    "\n",
    "# Eliminate Missing Values\n",
    "daily_return = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Normalize Data\n",
    "daily_return = z_score(daily_return)\n",
    "\n",
    "# Copy it for the future use\n",
    "daily_return1 = daily_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b5bef",
   "metadata": {},
   "source": [
    "### COVID-19 Confirmed Data\n",
    "\n",
    "This part of code imports __corresponding COVID-19 Confirmed Rate__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd70763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 Dataset\n",
    "\n",
    "states_url = \"https://covidtracking.com/api/states/daily\"\n",
    "us_url = \"https://covidtracking.com/api/us/daily\"\n",
    "case_threshold = 100\n",
    "\n",
    "cases = [\"confirmed\", \"deaths\", \"recovered\"]\n",
    "sheet = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_\"\n",
    "suffix = \"_global.csv\"\n",
    "df_list = []\n",
    "\n",
    "url_confirmed = sheet + \"confirmed\" + suffix\n",
    "\n",
    "df_confirmed = pd.read_csv(url_confirmed, header=0, escapechar=\"\\\\\")\n",
    "df_confirmed1 = df_confirmed.drop(columns=[\"Lat\", \"Long\"])\n",
    "df_confirmed = df_confirmed.drop(columns=[\"Lat\", \"Long\"])\n",
    "\n",
    "df_confirmed = df_confirmed.groupby(\"Country/Region\").agg(\"sum\").T\n",
    "df_confirmed1 = df_confirmed1.groupby(\"Province/State\").agg(\"sum\").T\n",
    "\n",
    "# Preprocess Data\n",
    "dates = df_confirmed.index.tolist()\n",
    "dates = date_convert(dates)\n",
    "US = df_confirmed[\"US\"].tolist()\n",
    "China = df_confirmed[\"China\"].tolist()\n",
    "Germany = df_confirmed[\"Germany\"].tolist()\n",
    "Japan = df_confirmed[\"Japan\"].tolist()\n",
    "UK = df_confirmed[\"United Kingdom\"].tolist()\n",
    "Korea = df_confirmed[\"Korea, South\"].tolist()\n",
    "Australia = df_confirmed[\"Australia\"].tolist()\n",
    "Austria = df_confirmed[\"Austria\"].tolist()\n",
    "Denmark = df_confirmed[\"Denmark\"].tolist()\n",
    "Greece = df_confirmed[\"Greece\"].tolist()\n",
    "Finland = df_confirmed[\"Finland\"].tolist()\n",
    "Ireland = df_confirmed[\"Ireland\"].tolist()\n",
    "Italy = df_confirmed[\"Italy\"].tolist()\n",
    "SouthAfrica = df_confirmed[\"South Africa\"].tolist()\n",
    "Spain = df_confirmed[\"Spain\"].tolist()\n",
    "Singapore = df_confirmed[\"Singapore\"].tolist()\n",
    "Russia = df_confirmed[\"Russia\"].tolist()\n",
    "NewZealand = df_confirmed[\"New Zealand\"].tolist()\n",
    "Canada = df_confirmed[\"Canada\"].tolist()\n",
    "France = df_confirmed[\"France\"].tolist()\n",
    "Netherlands = df_confirmed[\"Netherlands\"].tolist()\n",
    "Mexico = df_confirmed[\"Mexico\"].tolist()\n",
    "Brazil = df_confirmed[\"Brazil\"].tolist()\n",
    "Philippines = df_confirmed[\"Philippines\"].tolist()\n",
    "India = df_confirmed[\"India\"].tolist()\n",
    "Argentina = df_confirmed[\"Argentina\"].tolist()\n",
    "Indonesia = df_confirmed[\"Indonesia\"].tolist()\n",
    "Malaysia = df_confirmed[\"Malaysia\"].tolist()\n",
    "Israel = df_confirmed[\"Israel\"].tolist()\n",
    "Poland = df_confirmed[\"Poland\"].tolist()\n",
    "Afghanistan = df_confirmed[\"Afghanistan\"].tolist()\n",
    "\n",
    "data = [\n",
    "    US, China, Japan, \n",
    "    Korea, Australia, Austria, \n",
    "    Germany, UK, Denmark, \n",
    "    Greece, Italy, SouthAfrica, \n",
    "    Spain, Singapore, Russia, \n",
    "    NewZealand, Canada, France, \n",
    "    Netherlands, Mexico, Philippines, \n",
    "    India, Argentina, Indonesia, \n",
    "    Malaysia, Israel, Poland, \n",
    "    Brazil, Spain\n",
    "]\n",
    "\n",
    "# Country Codes\n",
    "country_codes = [\n",
    "    \"US\", \"CN\", \"JP\", \n",
    "    \"KR\", \"AU\", \"AT\", \n",
    "    \"DE\", \"GB\", \"DK\", \n",
    "    \"GR\", \"IT\", \"ZA\", \n",
    "    \"ES\", \"SG\", \"RU\", \n",
    "    \"NZ\", \"CA\", \"FR\", \n",
    "    \"NL\", \"MX\", \"PH\", \n",
    "    \"IN\", \"AR\", \"ID\", \n",
    "    \"MY\", \"IL\", \"PL\", \n",
    "    \"BR\", \"ES\"\n",
    "]\n",
    "\n",
    "daily_confirmed = pd.DataFrame(data, index=country_codes, columns=dates).T\n",
    "daily_confirmed = z_score(daily_confirmed)\n",
    "\n",
    "for code in country_codes:\n",
    "    population = CountryInfo(code).population()\n",
    "    daily_confirmed[code] = daily_confirmed[code].div(population, axis=0)\n",
    "\n",
    "daily_confirmed.index.name = 'Date'\n",
    "daily_confirmed1 = daily_confirmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331b69d",
   "metadata": {},
   "source": [
    "## Merge Dataframe \n",
    "\n",
    "Because I need to __merge dataframe__ to apply Principal Component Analysis as a means of calculating explained variance, I now merge two dataframes: __Daily Return__ and __Daily Confirmed__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Dates\n",
    "confirmed = daily_confirmed.index.tolist()\n",
    "returns = daily_return.index.tolist()\n",
    "\n",
    "# Build a list to include dates in common\n",
    "dates_common = []\n",
    "for date in returns:\n",
    "    date = (str(date)[:10])\n",
    "    if date in confirmed:\n",
    "        dates_common.append(date)\n",
    "\n",
    "# Only leave dates in common from daily_confirmed\n",
    "for date in daily_confirmed.index:\n",
    "    if date not in dates_common:\n",
    "        daily_confirmed = daily_confirmed.drop(date)\n",
    "\n",
    "# Only leave dates in common from daily_return\n",
    "daily_return_index = []\n",
    "for var in daily_return.index.tolist():\n",
    "    date = (str(var))[:10]\n",
    "    if date not in dates_common:\n",
    "        daily_return = daily_return.drop(var)\n",
    "    \n",
    "    else:\n",
    "        daily_return_index.append(str(date))\n",
    "\n",
    "daily_return.index = daily_return_index\n",
    "daily_return.index.name = 'Date'\n",
    "\n",
    "# Now, merge them in same index\n",
    "df_merged = pd.concat([daily_return, daily_confirmed], axis=1)\n",
    "\n",
    "# Normalize\n",
    "df_merged = z_score(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d4cdf",
   "metadata": {},
   "source": [
    "## Extraction of Five Principal Components\n",
    "\n",
    "I extract first 5 principal components. As written in __Kritzman et al. (2010)__, it represents how a limited number of principal components, which are orthogonal combinations of vectors, can explain total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(5).fit(df_merged)\n",
    "daily_return_factors = pd.Series(index=df_merged.columns, data=pca.components_[0])\n",
    "print(\"TOP 5 Principal Components\", pca.explained_variance_ratio_.round(2))\n",
    "variance_stock = pca.explained_variance_ratio_.cumsum().round(2)\n",
    "plt.plot(variance_stock)\n",
    "plt.title('Correlation', fontsize = 16)\n",
    "plt.xlabel('Number of PCs', fontsize = 14)\n",
    "plt.ylabel('Explained Variance (%)', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ff9c8",
   "metadata": {},
   "source": [
    "It appears that **even a single principal component, which means to maximize explained variance**, is able to explain nearly $70\\%$ of entire variance. Including next princpal component lets this model explain $90\\%$, which reflects Coronavirus and Price Fluctuation in the market are highly correlated in macro-perspective.\n",
    "\n",
    "## Eigenvectors\n",
    "\n",
    "Principal Component Analysis is essentially about reducing complex dimensionality of data by taking __orthogonal transformation__ of vectors. Each loading of principal component, which is `eigenvalue` whose sum of entire eigenvalues is equal to 1, as `unit vector`.\n",
    "\n",
    "$$\n",
    "u := \\min(\\frac{1}{n} \\sum_{i}^{n}(x_{i}^{T}x_{i} - (u_{1}^{T}x_{i})^{2}))\n",
    "$$\n",
    "\n",
    "Principal Component Analysis aims for minimizing total distance of a unit vector whose perpendicular distance is minimized as a result. And it is the eigenvector of the covariance matrix of $X$.\n",
    "\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "\n",
    "## PCA Loading: \"Degree of Impact\"\n",
    "\n",
    "Loading of first principal component could reveal how variables affected price fluctuation in 2020. Because first principal component, as proven above, explains nearly $70\\%$ of entire variance, I would like to assume that proportion of eigenvalues suggests how each indice was influenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4373f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_dr = PCA(1).fit(daily_return1)\n",
    "daily_return_factors = pd.Series(index=daily_return1.columns, data=pca_dr.components_[0])\n",
    "\n",
    "daily_return_factors.nlargest(5).plot.bar()\n",
    "plt.title('Best Indices', fontsize=16)\n",
    "plt.xlabel('Indices', fontsize=14)\n",
    "plt.ylabel('PCA Loadings', fontsize = 14)\n",
    "# plt.savefig('Best Indices.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51664b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_return_factors.nsmallest(5).plot.bar()\n",
    "plt.title('Worst Indices', fontsize=16)\n",
    "plt.xlabel('Indices', fontsize=14)\n",
    "plt.ylabel('PCA Loadings', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b97cf",
   "metadata": {},
   "source": [
    "It appears that China, Singapore, Great Britain (FTSE 100), and US (Dow) are least negatively affected whereas South Africa, Ireland, France, Canada, and Britain are most negatively affected in 2020. It appears that even first principal component well summarizes the correlation between COVID-19 confirmed rate and price fluctuation. The countries who managed COVID-19 well, China and Singapore, seem to be least affected whereas countries who could not manage it well seem to take worse impact from it. While United States and Great Britain are not the ones who managed COVID-19 well, it appears that their very skillful monetary policy led them to minimize financial impact. Furthermore, I included a large number of indices of United States and Great Britain, but only those with \"big\" companies (U.S. Dow and FTSE 100) are ranked at top 5 indices - which means polarization of market, often accelerated by powerful monetary policy, is inferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92692866",
   "metadata": {},
   "source": [
    "# Final Note\n",
    "\n",
    "The experiment intends to test my null hypothesis that a single principal component is able to explain almost entire variance to explain price fluctuation in 2020. However, the result proves that uni-variate model should not have sufficient explanatory power even in the year of 2020. First of all, the first principal component has an explained variance of $66\\%$, which is quite limited. Also, comparing degrees of impact well explains that most severly damaged countries, especially the United States, fail to appear in the right spot. Great Britain and United States, two countries who are most damaged by COVID-19, are top 4 and 5 countries in \"positively influenced\" in this period, whereas South Africa, whose capacity to defend COVID-19 was much stronger, is ranked as the most negatively affected country in 2020, according to the research. I suppose that countries who hold capacity to conduct strong monetary policy, such as the United States and Great Britain, well offset the damage caused by COVID-19 whereas countries who lack in this competence are hardly harmed by this occurence. As a conclusion, I found that my original $H_{0}$ was rejected; there are still more than one factor to explain variance around financial market, and even a year of 2020 is not an exception. I believe my future econometric works should abide by the assumption that financial market should be understood in multivariate models.\n",
    "\n",
    "# Bibliography\n",
    "\n",
    "[1] Kritzman, Mark and Li, Yuanzhen and Page, Sebastien and Rigobon, Roberto, Principal Components as a Measure of Systemic Risk (June 30, 2010). MIT Sloan Research Paper No. 4785-10, https://doi.org/10.3905/jpm.2011.37.4.112, Available at SSRN: https://ssrn.com/abstract=1633027 or http://dx.doi.org/10.2139/ssrn.1633027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78e124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
