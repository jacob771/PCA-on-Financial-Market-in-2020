{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f5dd42",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A hypothesis behind econometric analysis is that financial market is multivariate. However, this experiment intends to challenge this hypothesis, as COVID-19 influenced almost every part of our lives in 2020. Because one single factor (COVID-19) caused very powerful impact on global economy, price fluctuation of global stock market may largely be explained by COVID-19 spread rate. I do not intend to hypothesize that $100\\%$ of variance can be explained by the spread of COVID-19; however, I would like to assume that an analysis could explain approximately $80\\%$ of variance. Because I want to understand correlation between market indices and COVID-19 confirmed rate. I would like to combine a number of data and would like to calculate explained rate of variance in each principal component.\n",
    "\n",
    "# Methodology\n",
    "\n",
    "## Explained Variance of First Principal Component\n",
    "\n",
    "In PCA, ${\\bf W},{\\bf Z}$ can be found using either an eigenvalue decomposition or the SVD. Suppose the matrix ${\\bf X}$ is centered so that\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i = 1}^n {\\bf x}_i = \\vec{0} \\in \\mathbb{R}^p\n",
    "$$\n",
    "\n",
    "The covariance matrix is defined to be ${\\bf C} = {\\bf XX}^T$.  The eigen-decomposition of ${\\bf C} = {\\bf W\\Sigma W}^T$ gives the matrices ${\\bf W}$ and ${\\bf Z} = {\\bf \\Sigma W}^T$.  Here\n",
    "\n",
    "$$\n",
    "{\\bf \\Sigma} = \\begin{bmatrix}\n",
    "\\sigma_1^2 & & \\\\\n",
    " & \\ddots & \\\\\n",
    " & & \\sigma_p^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is the diagonal matrix with eigenvalues $\\sigma_i^2$.  The proportion of explained variance by our low-dimensional projection is defined to be\n",
    "\n",
    "$$\n",
    "\\mathrm{PV}(q) := \\frac{\\sum_{i=1}^q \\sigma_i^2}{\\sum_{i=1}^p \\sigma_i^2}\n",
    "$$\n",
    "\n",
    "I suppose that $80\\%$ of variance could be explained by the first principal component. In other words, I suppose that 40 global stock indices and COVID-19 confirmed rate of corresponding country may move together by the degree of $80\\%$.\n",
    "\n",
    "# Experiment\n",
    "\n",
    "## Import Libraries and Define Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d23414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 4800x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Packages\n",
    "\n",
    "import yfinance as yf\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from countryinfo import CountryInfo\n",
    "from sklearn.decomposition import PCA\n",
    "from datapackage import Package\n",
    "\n",
    "today = datetime.today()\n",
    "yesterday = str(today - timedelta(2))[:10]\n",
    "\n",
    "# apply the z-score method in Pandas using the .mean() and .std() methods\n",
    "def z_score(df):\n",
    "    # copy the dataframe\n",
    "    df_std = df.copy()\n",
    "    # apply the z-score method\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "        \n",
    "    return df_std\n",
    "\n",
    "# Convert Date\n",
    "def date_convert(dates):\n",
    "    dates_return = []\n",
    "    \n",
    "    for date in dates:\n",
    "        date = date.split(\"/\")\n",
    "        year = '20' + str(date[2])\n",
    "        month = str(date[0])\n",
    "        day = str(date[1])\n",
    "        \n",
    "        if int(month) < 10:\n",
    "            month = '0' + month\n",
    "        \n",
    "        if int(day) < 10:\n",
    "            day = '0' + day\n",
    "        \n",
    "        date = year + \"-\" + month + \"-\" + day\n",
    "        dates_return.append(date)\n",
    "    \n",
    "    return dates_return\n",
    "\n",
    "start = \"2020-01-22\"\n",
    "end = \"2021-01-01\"\n",
    "\n",
    "# Matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('max_rows', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 12)\n",
    "plt.style.use('seaborn-pastel')\n",
    "plt.rcParams['lines.linewidth'] = 1\n",
    "plt.figure(dpi=300)\n",
    "plt.rcParams['lines.color'] = 'b'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba85323",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "### Import Stock Indices\n",
    "\n",
    "This part of code imports __40 Market Indices__ that represent major financial market in the globe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cfbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    }
   ],
   "source": [
    "# Import 40 Market Indices\n",
    "\n",
    "BUK100P = yf.download(\"^BUK100P\", start, end)['Adj Close'].to_frame()\n",
    "SPY = yf.download(\"SPY\", start, end)['Adj Close'].to_frame()\n",
    "Singapore = yf.download(\"^STI\", start, end)['Adj Close'].to_frame()\n",
    "Dow = yf.download(\"^DJI\", start, end)['Adj Close'].to_frame()\n",
    "Nasdaq = yf.download(\"^IXIC\", start, end)['Adj Close'].to_frame()\n",
    "FTSE100 = yf.download(\"^FTSE\", start, end)['Adj Close'].to_frame()\n",
    "FTSE250 = yf.download(\"^FTSE\", start, end)['Adj Close'].to_frame()\n",
    "FTSE350 = yf.download(\"^FTLC\", start, end)['Adj Close'].to_frame()\n",
    "FTAI = yf.download(\"^FTAI\", start, end)['Adj Close'].to_frame()\n",
    "N225 = yf.download(\"^N225\", start, end)['Adj Close'].to_frame()\n",
    "N500 = yf.download(\"^N500\", start, end)['Adj Close'].to_frame()\n",
    "N1000 = yf.download(\"^N1000\", start, end)['Adj Close'].to_frame()\n",
    "HSI = yf.download(\"^HSI\", start, end)['Adj Close'].to_frame()\n",
    "Taiwan = yf.download(\"^TWII\", start, end)['Adj Close'].to_frame()\n",
    "SSE = yf.download(\"000001.SS\", start, end)['Adj Close'].to_frame()\n",
    "Shenzhen = yf.download(\"399001.SZ\", start, end)['Adj Close'].to_frame()\n",
    "DAX = yf.download(\"^GDAXI\", start, end)['Adj Close'].to_frame()\n",
    "France = yf.download(\"^FCHI\", start, end)['Adj Close'].to_frame()\n",
    "Indonesia = yf.download(\"^JKSE\", start, end)['Adj Close'].to_frame()\n",
    "PSEI = yf.download(\"PSEI.PS\", start, end)['Adj Close'].to_frame()\n",
    "AORD = yf.download(\"^AORD\", start, end)['Adj Close'].to_frame()\n",
    "AXJO = yf.download(\"^AXJO\", start, end)['Adj Close'].to_frame()\n",
    "AXKO = yf.download(\"^AXKO\", start, end)['Adj Close'].to_frame()\n",
    "kospi = yf.download(\"^KS11\", start, end)['Adj Close'].to_frame()\n",
    "India = yf.download(\"^BSESN\", start, end)['Adj Close'].to_frame()\n",
    "NZ50 = yf.download(\"^NZ50\", start, end)['Adj Close'].to_frame()\n",
    "XAX = yf.download(\"^XAX\", start, end)['Adj Close'].to_frame()\n",
    "RUI = yf.download(\"^RUI\", start, end)['Adj Close'].to_frame()\n",
    "RUT = yf.download(\"^RUT\", start, end)['Adj Close'].to_frame()\n",
    "RUA = yf.download(\"^RUA\", start, end)['Adj Close'].to_frame()\n",
    "GSPTSE = yf.download(\"^GSPTSE\", start, end)['Adj Close'].to_frame()\n",
    "N100 = yf.download(\"^N100\", start, end)['Adj Close'].to_frame()\n",
    "N150 = yf.download(\"^N150\", start, end)['Adj Close'].to_frame()\n",
    "BFX = yf.download(\"^BFX\", start, end)['Adj Close'].to_frame()\n",
    "IMOEX = yf.download(\"IMOEX.ME\", start, end)['Adj Close'].to_frame()\n",
    "MERV = yf.download(\"^MERV\", start, end)['Adj Close'].to_frame()\n",
    "TA125 = yf.download(\"^TA125.TA\", start, end)['Adj Close'].to_frame()\n",
    "JN0U = yf.download(\"^JN0U.JO\", start, end)['Adj Close'].to_frame()\n",
    "AEX = yf.download(\"^AEX\", start, end)['Adj Close'].to_frame()\n",
    "ATOI = yf.download(\"^ATOI\", start, end)['Adj Close'].to_frame()\n",
    "BVSP = yf.download(\"^BVSP\", start, end)['Adj Close'].to_frame()\n",
    "MIB = yf.download(\"FTSEMIB.MI\", start, end)['Adj Close'].to_frame()\n",
    "ATX = yf.download(\"^ATX\", start, end)['Adj Close'].to_frame()\n",
    "ISEQ = yf.download(\"^ISEQ\", start, end)['Adj Close'].to_frame()\n",
    "NSEI = yf.download(\"^NSEI\", start, end)['Adj Close'].to_frame()\n",
    "MXX = yf.download(\"^MXX\", start, end)['Adj Close'].to_frame()\n",
    "SSMI = yf.download(\"^SSMI\", start, end)['Adj Close'].to_frame()\n",
    "STOXX50E = yf.download(\"^STOXX50E\", start, end)['Adj Close'].to_frame()\n",
    "MDAXI = yf.download(\"^MDAXI\", start, end)['Adj Close'].to_frame()\n",
    "SDAXI = yf.download(\"^SDAXI\", start, end)['Adj Close'].to_frame()\n",
    "HSCC = yf.download(\"^HSCC\", start, end)['Adj Close'].to_frame()\n",
    "HSCE = yf.download(\"^HSCE\", start, end)['Adj Close'].to_frame()\n",
    "KLSE = yf.download(\"^KLSE\", start, end)['Adj Close'].to_frame()\n",
    "\n",
    "# Transform into Dataframe\n",
    "df = pd.concat([\n",
    "    BUK100P, Dow, Nasdaq, FTSE100, FTSE250, FTAI, N225, SSE, Shenzhen, \n",
    "    DAX, France, Indonesia, PSEI, AXKO, kospi, NZ50, RUI, RUT, RUA, \n",
    "    GSPTSE, N100, N150, BFX, IMOEX, MERV, TA125, JN0U, SPY, Singapore, \n",
    "    AEX, ATOI, BVSP, MIB, ATX, ISEQ, MXX, STOXX50E, MDAXI, SDAXI, KLSE], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Set Columns\n",
    "# Columns include Country Code, so that I can match country to COVID-19 confirmed rate.\n",
    "df.columns=[\n",
    "    'US-Dow', 'US-Nasdaq', 'GB-FTSE100', 'GB-FTSE250', 'GB-FTAI',\n",
    "    'GB-BUK100P', 'JP-N225', 'CN-SSE', 'CN-Shenzhen', 'DE-DAX', \n",
    "    'FR-FCHI', 'ID-JKSE', 'PH-PSEI', 'AU-AXKO', 'KR-KSII', 'NZ-NZ50',\n",
    "    'US-RUI', 'US-RUT', 'US-RUA', 'CA-GSPTSE', 'FR-N100', 'FR-N150', \n",
    "    'BE-BFS', 'RU-IMOEX', 'AR-MERV', 'IL-TA125', 'ZA-JN0U', 'US-SPX', \n",
    "    'SG-STI', 'NL-AEX', 'AU-ATOI', 'BR-BVSP', 'IT-MIB', 'AT-ATX', 'IE-ISEQ',\n",
    "    'MX-MXX', 'DE-Stoxx50E', 'DE-MDAXI', 'DE-SDAXI', 'MY-KLSE']\n",
    "\n",
    "# Eliminate Missing Values\n",
    "daily_return = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Normalize Data\n",
    "daily_return = z_score(daily_return)\n",
    "\n",
    "# Copy it for the future use\n",
    "daily_return1 = daily_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b5bef",
   "metadata": {},
   "source": [
    "### COVID-19 Confirmed Data\n",
    "\n",
    "This part of code imports __corresponding COVID-19 Confirmed Rate__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd70763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 Dataset\n",
    "\n",
    "states_url = \"https://covidtracking.com/api/states/daily\"\n",
    "us_url = \"https://covidtracking.com/api/us/daily\"\n",
    "case_threshold = 100\n",
    "\n",
    "cases = [\"confirmed\", \"deaths\", \"recovered\"]\n",
    "sheet = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_\"\n",
    "suffix = \"_global.csv\"\n",
    "df_list = []\n",
    "\n",
    "url_confirmed = sheet + \"confirmed\" + suffix\n",
    "\n",
    "df_confirmed = pd.read_csv(url_confirmed, header=0, escapechar=\"\\\\\")\n",
    "df_confirmed1 = df_confirmed.drop(columns=[\"Lat\", \"Long\"])\n",
    "df_confirmed = df_confirmed.drop(columns=[\"Lat\", \"Long\"])\n",
    "\n",
    "df_confirmed = df_confirmed.groupby(\"Country/Region\").agg(\"sum\").T\n",
    "df_confirmed1 = df_confirmed1.groupby(\"Province/State\").agg(\"sum\").T\n",
    "\n",
    "# Preprocess Data\n",
    "dates = df_confirmed.index.tolist()\n",
    "dates = date_convert(dates)\n",
    "US = df_confirmed[\"US\"].tolist()\n",
    "China = df_confirmed[\"China\"].tolist()\n",
    "Germany = df_confirmed[\"Germany\"].tolist()\n",
    "Japan = df_confirmed[\"Japan\"].tolist()\n",
    "UK = df_confirmed[\"United Kingdom\"].tolist()\n",
    "Korea = df_confirmed[\"Korea, South\"].tolist()\n",
    "Australia = df_confirmed[\"Australia\"].tolist()\n",
    "Austria = df_confirmed[\"Austria\"].tolist()\n",
    "Denmark = df_confirmed[\"Denmark\"].tolist()\n",
    "Greece = df_confirmed[\"Greece\"].tolist()\n",
    "Finland = df_confirmed[\"Finland\"].tolist()\n",
    "Ireland = df_confirmed[\"Ireland\"].tolist()\n",
    "Italy = df_confirmed[\"Italy\"].tolist()\n",
    "SouthAfrica = df_confirmed[\"South Africa\"].tolist()\n",
    "Spain = df_confirmed[\"Spain\"].tolist()\n",
    "Singapore = df_confirmed[\"Singapore\"].tolist()\n",
    "Russia = df_confirmed[\"Russia\"].tolist()\n",
    "NewZealand = df_confirmed[\"New Zealand\"].tolist()\n",
    "Canada = df_confirmed[\"Canada\"].tolist()\n",
    "France = df_confirmed[\"France\"].tolist()\n",
    "Netherlands = df_confirmed[\"Netherlands\"].tolist()\n",
    "Mexico = df_confirmed[\"Mexico\"].tolist()\n",
    "Brazil = df_confirmed[\"Brazil\"].tolist()\n",
    "Philippines = df_confirmed[\"Philippines\"].tolist()\n",
    "India = df_confirmed[\"India\"].tolist()\n",
    "Argentina = df_confirmed[\"Argentina\"].tolist()\n",
    "Indonesia = df_confirmed[\"Indonesia\"].tolist()\n",
    "Malaysia = df_confirmed[\"Malaysia\"].tolist()\n",
    "Israel = df_confirmed[\"Israel\"].tolist()\n",
    "Poland = df_confirmed[\"Poland\"].tolist()\n",
    "Afghanistan = df_confirmed[\"Afghanistan\"].tolist()\n",
    "\n",
    "data = [\n",
    "    US, China, Japan, \n",
    "    Korea, Australia, Austria, \n",
    "    Germany, UK, Denmark, \n",
    "    Greece, Italy, SouthAfrica, \n",
    "    Spain, Singapore, Russia, \n",
    "    NewZealand, Canada, France, \n",
    "    Netherlands, Mexico, Philippines, \n",
    "    India, Argentina, Indonesia, \n",
    "    Malaysia, Israel, Poland, \n",
    "    Brazil, Spain\n",
    "]\n",
    "\n",
    "# Country Codes\n",
    "country_codes = [\n",
    "    \"US\", \"CN\", \"JP\", \n",
    "    \"KR\", \"AU\", \"AT\", \n",
    "    \"DE\", \"GB\", \"DK\", \n",
    "    \"GR\", \"IT\", \"ZA\", \n",
    "    \"ES\", \"SG\", \"RU\", \n",
    "    \"NZ\", \"CA\", \"FR\", \n",
    "    \"NL\", \"MX\", \"PH\", \n",
    "    \"IN\", \"AR\", \"ID\", \n",
    "    \"MY\", \"IL\", \"PL\", \n",
    "    \"BR\", \"ES\"\n",
    "]\n",
    "\n",
    "daily_confirmed = pd.DataFrame(data, index=country_codes, columns=dates).T\n",
    "daily_confirmed = z_score(daily_confirmed)\n",
    "\n",
    "for code in country_codes:\n",
    "    population = CountryInfo(code).population()\n",
    "    daily_confirmed[code] = daily_confirmed[code].div(population, axis=0)\n",
    "\n",
    "daily_confirmed.index.name = 'Date'\n",
    "daily_confirmed1 = daily_confirmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331b69d",
   "metadata": {},
   "source": [
    "## Merge Dataframe \n",
    "\n",
    "Because I need to __merge dataframe__ to apply Principal Component Analysis as a means of calculating explained variance, I now merge two dataframes: __Daily Return__ and __Daily Confirmed__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Dates\n",
    "confirmed = daily_confirmed.index.tolist()\n",
    "returns = daily_return.index.tolist()\n",
    "\n",
    "# Build a list to include dates in common\n",
    "dates_common = []\n",
    "for date in returns:\n",
    "    date = (str(date)[:10])\n",
    "    if date in confirmed:\n",
    "        dates_common.append(date)\n",
    "\n",
    "# Only leave dates in common from daily_confirmed\n",
    "for date in daily_confirmed.index:\n",
    "    if date not in dates_common:\n",
    "        daily_confirmed = daily_confirmed.drop(date)\n",
    "\n",
    "# Only leave dates in common from daily_return\n",
    "daily_return_index = []\n",
    "for var in daily_return.index.tolist():\n",
    "    date = (str(var))[:10]\n",
    "    if date not in dates_common:\n",
    "        daily_return = daily_return.drop(var)\n",
    "    \n",
    "    else:\n",
    "        daily_return_index.append(str(date))\n",
    "\n",
    "daily_return.index = daily_return_index\n",
    "daily_return.index.name = 'Date'\n",
    "\n",
    "# Now, merge them in same index\n",
    "df_merged = pd.concat([daily_return, daily_confirmed], axis=1)\n",
    "\n",
    "# Normalize\n",
    "df_merged = z_score(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d4cdf",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "## Extraction of Five Principal Components\n",
    "\n",
    "I extract first 5 principal components. They represent how a limited number of principal components, which are orthogonal combinations of vectors, can explain total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(5).fit(df_merged)\n",
    "daily_return_factors = pd.Series(index=df_merged.columns, data=pca.components_[0])\n",
    "print(\"TOP 5 Principal Components:\", pca.explained_variance_ratio_.round(2))\n",
    "variance_stock = pca.explained_variance_ratio_.cumsum().round(2)\n",
    "plt.plot(variance_stock)\n",
    "plt.title('Correlation', fontsize = 16)\n",
    "plt.xlabel('Number of PCs', fontsize = 14)\n",
    "plt.ylabel('Explained Variance (%)', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad673e0",
   "metadata": {},
   "source": [
    "First principal component explains $66\\%$ of entire variance, which means $H_{0}$ is accepted. If we take second principal component into account, it combinedly explains $80\\%$ of variance. It appears that COVID-19 is very powerful variable to explain financial market's fluctuation. However, it appears that it still does not explain $80\\%$ of entire variance, which means that its impact is still limited to challenge multivariate assumption.\n",
    "\n",
    "# Comparison of Eigenvalues\n",
    "\n",
    "While [explained variance methodology](#Methodology) proves that COVID-19 Confirmed Rate is very limited variable to explain financial fluctuation alone, I still believe \n",
    "\n",
    "## Eigenvalues\n",
    "\n",
    "Principal Component Analysis is essentially about reducing complex dimensionality of data by taking __orthogonal transformation__ of vectors. Each loading of principal component, which is `eigenvalue` whose sum of entire eigenvalues is equal to 1, as `unit vector`.\n",
    "\n",
    "$$\n",
    "u := \\min(\\frac{1}{n} \\sum_{i}^{n}(x_{i}^{T}x_{i} - (u_{1}^{T}x_{i})^{2}))\n",
    "$$\n",
    "\n",
    "Principal Component Analysis aims for minimizing total distance of a unit vector whose perpendicular distance is minimized as a result. And it is the eigenvector of the covariance matrix of $X$.\n",
    "\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "\n",
    "## PCA Loading: \"Degree of Impact\"\n",
    "\n",
    "Loading of first principal component could reveal how variables affected price fluctuation in 2020. Because first principal component, as proven above, explains nearly $70\\%$ of entire variance, I would like to assume that proportion of eigenvalues suggests how each indice was influenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4373f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_dr = PCA(1).fit(daily_return1)\n",
    "daily_return_factors = pd.Series(index=daily_return1.columns, data=pca_dr.components_[0])\n",
    "\n",
    "daily_return_factors.nlargest(5).plot.bar()\n",
    "plt.title('Best Indices', fontsize=16)\n",
    "plt.xlabel('Indices', fontsize=14)\n",
    "plt.ylabel('PCA Loadings', fontsize = 14)\n",
    "# plt.savefig('Best Indices.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51664b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_return_factors.nsmallest(5).plot.bar()\n",
    "plt.title('Worst Indices', fontsize=16)\n",
    "plt.xlabel('Indices', fontsize=14)\n",
    "plt.ylabel('PCA Loadings', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b97cf",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "PCA reveals that COVID-19 does not hold enough explanatory power to explain price fluctuation in financial market. The first principal component only explains $66\\%$ of full variance, and comparing eigenvalues implies the United States and Great Britain are the examples that deviate from correlation between COVID-19 confirmed rate and price fluctuation of global market indices. In specific, we have values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most Positively Influenced Indices Are:\")\n",
    "print(daily_return_factors.nlargest(5).index.tolist())\n",
    "print(\"\")\n",
    "print(\"Degree of Impact is\", daily_return_factors.nlargest(5).values.tolist())\n",
    "print(\"\")\n",
    "\n",
    "print(\"Most Negatively Influenced Indices Are:\")\n",
    "print(daily_return_factors.nsmallest(5).index.tolist())\n",
    "print(\"\")\n",
    "print(\"Degree of Impact is\", daily_return_factors.nsmallest(5).values.tolist())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08d3fa",
   "metadata": {},
   "source": [
    "Five most __positively__ influenced indices belong to China, Singapore, Great Britain, and United States. Five most __negatively__ influenced indices belong to South Africa, Spain, France, Canada, and Brazil. Top 5 indices mostly belong to countries who contained COVID-19 well whereas worst 5 ones mostly belong to those who could not control spread of COVID-19 well. However, it appears that __Great Britain__ and __United States__ are anomalies, as they are most negatively influenced countries with very powerful spread of COVID-19 while they are still listed as best 5 indices. I assume that another factor includes ability to implement monetary policy. United States and Great Britain hold very skillful professionals who could utilize their capital to develop very efficient and appropriate monetary policy. As a consequence, I believe that COVID-19 is very powerful variable that partly explains variance of financial market. However, it is still not sufficient to build univariate model that provides significant explanatory power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
